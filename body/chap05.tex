\chapter{基于多模态先验引导的帧间一致性优化算法研究}
\label{cha:fifthsection}

\section{引言}

时空动作检测任务的核心在于同时解决“是什么”（动作分类）和“在哪里”（时空定位）
的问题。为了提高模型对复杂体育动作的检测能力，第三章提出了一种基于双粒度语义
增强的体育时空动作检测算法，通过构建外部知识库引入了动作的深层语义信息，有效
解决了视觉相似动作的混淆问题 。然而，该方法侧重于语义特征的辨析，在处理大位
移、快节奏的运动场景时，若缺乏对时空动力学的精细建模，容易出现定位偏差。为了
增强模型对动作时序变化的适应能力，第四章设计了一种基于时空一致性建模的方法，
通过动作感知模块（AAM）和自适应采样机制，显著提升了模型在复杂运动下的跟踪与
定位鲁棒性 。尽管第四章的方法在时空建模上表现优异，但在体育场景下，动作的时
序演变往往受到复杂规则和技术规范的严苛约束。单纯依赖视觉特征激发的时空一致性
建模存在局限性：当视频中出现严重遮挡、背景干扰或多目标相似运动时，仅凭视觉感知
的初始查询（Query）容易发生“语义漂移”（Semantic Drift），即虽然跟踪到了目标，
但无法确定该目标是否符合特定的动作语义（例如将“无球跑动”误检为“带球突破”）。
此时，视觉特征不再可靠，必须引入隐含在规则背后的高层语义信息作为“导航”。因此，
本章提出了一种基于多模态先验引导的帧间一致性优化算法
（Multimodal Prior-guided Temporal Consistency Optimization, MPTCO）。
该算法并非简单的模块叠加，而是结合前两章的相关工作，构建了一种“语义引导+时空执行”的
协同机制。

根据第三章的实验结果分析，利用多模态大模型所提取的跨模态联合特征，对
分类性能的提升尤为显著；而第四章的实验结果表明，时空一致性建模对定位性能的提升
更为明显。因此，本章旨在将第三章构建的多模态知识库作为“先验信息”，注入到第四章
的动作感知模块（AAM）中。通过多模态提示（Multimodal Prompt）引导动作管查询的
初始化，使得查询向量在生成之初便兼具“视觉敏感性”和“语义判别性”，从而在后续的
时空解码过程中保持高度的帧间一致性，进一步提升模型在复杂体育场景下的理解和检测性能。
\section{多模态引导的一致性网络架构}

本章提出的MPTCO算法架构如图5.1所示。该框架是一个端到端的整体网络，
它有机融合了第三章的语义知识库与第四章的时空检测网络。整体架构主要由
三个关键部分组成：
\begin{enumerate}
    \item \textbf{多模态先验知识库（Multimodal Prior Knowledge Base）：} 
    复用第三章构建的“场景-文本”与“运动-文本”双粒度知识库。该模块负责为输入的
    视频片段检索匹配的文本语义原型（Semantic Prototypes），作为高层的语义
    指导信号。这些原型向量蕴含了体育动作的规则定义与环境上下文信息。
    
    \item \textbf{语义提示动作感知模块（Semantic-Prompted AAM）：} 
    这是本章的核心结合点。不同于第四章仅利用视觉特征生成查询，改进后的AAM模块
    引入了“早期融合”策略。它接收来自视频骨干网络的时空视觉特征和来自知识库的
    语义原型，通过注意力机制将语义信息注入到动作管查询（Action Tube Queries）
    的初始化过程中。生成的查询向量因此被赋予了明确的语义目标。

    \item \textbf{时空一致性解码器（Spatiotemporal Consistency Decoder）：} 
    沿用第四章设计的高效解码结构。包含“动作引导的自适应采样模块（AGAS）”和“解耦
    时空交叉注意力模块（DSTA）”。由于输入的查询向量已经经过了多模态语义的“预热”与
    引导，该解码器能够更精准地在时空特征图中采样与动作定义高度相关的区域，即使在
    视觉特征模糊的情况下，也能依靠语义先验维持时空轨迹的连续性。

\end{enumerate}

这种架构设计保证了语义信息能够作为一种强先验（Strong Prior），
在检测流程的源头对时空建模进行校准，实现了语义增强与一致性建模的深度协同。

\section{多模态提示驱动的查询初始化}

在基于查询（Query-based）的时空动作检测范式中，查询向量的质量直接决定了
后续检测的性能。第四章中的动作感知模块（AAM）通过在全局时空特征上应用卷积
和注意力机制来捕捉潜在的动作实例 ，这本质上是一种“视觉驱动”的初始化方式。
然而，在体育场景中，许多动作在视觉上具有极高的相似性，仅靠视觉特征难以在
初始化阶段就区分出正确的关注对象。为了解决这一问题，本节提出了多模态提示
驱动的查询初始化机制。其核心思想是利用第三章获取的语义原型作为“提示（Prompt）”，
对视觉感知的查询进行特征重构。具体而言，假设AAM模块从时空特征图 $S$ 中
提取出的原始视觉动作特征为 $F_{vis} \in \mathbb{R}^{N \times C}$，
其中 $N$ 为预设的查询数量，$C$ 为特征维度。同时，从第三章的双粒度知识库中，
我们可以获取当前动作类别对应的语义原型集合 
$P_{text} \in \mathbb{R}^{K \times C}$（$K$ 为类别数）。
为了将语义先验融入初始化过程，我们设计了一个语义注入操作。
首先，计算视觉特征 $F_{vis}$ 与语义原型 $P_{text}$ 之间的相关性，
以检索最匹配的语义提示信息。随后，通过残差连接的方式将语义信息融合到视觉特征中，
公式定义如下：
\begin{equation}
    Q_{init} = \text{LayerNorm}(F_{vis} + \alpha \cdot \text{CrossAttention}(F_{vis}, P_{text}, P_{text}))
\end{equation}
其中，$\alpha$ 是一个可学习的平衡系数，用于调节语义先验的权重。
$\text{CrossAttention}$ 操作以视觉特征作为查询（Query），以语义原型作为
键（Key）和值（Value）。通过这种方式生成的初始化查询 $Q_{init}$，不仅保留了
图像中物体的位置和运动信息（来自 $F_{vis}$），还融合了动作的定义和规则信息
（来自 $P_{text}$）。例如，在检测“篮球投篮”时，视觉特征可能捕捉到了“跳跃的人”，
而注入的语义原型会补充“手臂上举”、“面向篮筐”等高层特征。这种带有语义提示的查询
向量 $Q_{init}$ 随后被送入第四章所述的时序扩展模块，生成贯穿整个视频片段的动作管
查询 $Q_{tube}$。由于初始查询已经具备了明确的语义指向性，后续的AGAS模块在进行
自适应采样时，能够更有效地过滤掉背景噪声和无关动作的干扰，从而显著提升帧间一致性建模
的鲁棒性。

\section{实验结果与分析}
\subsection{实验设置}
\subsection{对比实验结果}
\subsection{消融实验结果}
\section{本章小结}






